{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what youâ€™re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    # unpack from_coord & to_coord\n",
    "    lon1, lat1 = from_coord\n",
    "    lon2, lat2 = to_coord\n",
    "    # set radius of earth\n",
    "    earth_r = 6371\n",
    "    # convert distance between lats & lons to radians\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    # solve for distance using haversine function \n",
    "    a = (math.sin(dlat / 2) ** 2 +\n",
    "         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n",
    "         math.sin(dlon / 2) **2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    dis = earth_r * c\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    # add new column distance\n",
    "    dataframe['distance'] = np.nan\n",
    "    # iterate over all rows and add calculated distance\n",
    "    for index, row in dataframe.iterrows():\n",
    "        dataframe.loc[index,'distance'] = calculate_distance((row[\"pickup_longitude\"],row[\"pickup_latitude\"]),(row[\"dropoff_longitude\"],row[\"dropoff_latitude\"]))                                       \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "def find_taxi_csv_urls():\n",
    "    html = get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')    \n",
    "    urls = [link.get('href') for link in soup.find_all(title=\"Yellow Taxi Trip Records\")]\n",
    "    expected=[]\n",
    "    pattern1 = re.compile(r\".*2009-\\d\\d.parquet\")\n",
    "    pattern2 = re.compile(r\".*201[0-4]-\\d\\d.parquet\")\n",
    "    pattern3 = re.compile(r\".*2015-0[1-6].parquet\")\n",
    "    for link in urls:\n",
    "        match= pattern1.search(link)\n",
    "        if match:\n",
    "            expected.append(match.string)\n",
    "    for link in urls:\n",
    "        match= pattern2.search(link)\n",
    "        if match:\n",
    "            expected.append(match.string)\n",
    "    for link in urls:\n",
    "        match= pattern3.search(link)\n",
    "        if match:\n",
    "            expected.append(match.string)\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    df = pd.read_parquet(url, engine='pyarrow')\n",
    "    #normalizing column names\n",
    "    if \"tpep_pickup_datetime\" in df.columns:\n",
    "        df.rename(columns={\"tpep_pickup_datetime\": \"pickup_datetime\", \n",
    "                           \"tpep_dropoff_datetime\":\"dropoff_datetime\"}, inplace=True)\n",
    "    \n",
    "    if \"Start_Lon\" in df.columns:\n",
    "        df.rename(columns={\"Start_Lon\": \"pickup_longitude\",\n",
    "                          \"End_Lon\": \"dropoff_longitude\",\n",
    "                          \"Start_Lat\": \"pickup_latitude\",\n",
    "                          \"End_Lat\": \"dropoff_latitude\",\n",
    "                           \"Trip_Pickup_DateTime\": \"pickup_datetime\", \n",
    "                            \"Tip_Amt\": \"tip_amount\"}, inplace=True)\n",
    "    \n",
    "    #extract longitude & latitude from location ids\n",
    "    if \"PULocationID\" in df.columns:\n",
    "        GeoSeries= gpd.read_file(filename=\"taxi_zones.zip\",engine=\"pyogrio\")\n",
    "        g=GeoSeries.to_crs(epsg=4326)\n",
    "        g[\"longtitude\"]=g.geometry.centroid.x\n",
    "        g[\"latitude\"]=g.geometry.centroid.y\n",
    "        df[\"pickup_latitude\"]=df[\"PULocationID\"].map(g[\"latitude\"])\n",
    "        df[\"pickup_longitude\"]=df[\"PULocationID\"].map(g[\"longtitude\"])\n",
    "        df[\"dropoff_latitude\"]=df[\"DOLocationID\"].map(g[\"latitude\"])\n",
    "        df[\"dropoff_longitude\"]=df[\"DOLocationID\"].map(g[\"longtitude\"])\n",
    "    \n",
    "    # only select columns we need to keep\n",
    "    cols_to_keep = [\"pickup_datetime\",\"pickup_longitude\",\"dropoff_longitude\",\"pickup_latitude\",\"dropoff_latitude\",\"tip_amount\"]\n",
    "    df = df[cols_to_keep]\n",
    "    \n",
    "    # convert pickup_datetime column to datetime\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df.pickup_datetime)\n",
    "    \n",
    "    #removing data outside new york coords\n",
    "    df = df[(df.pickup_longitude>=NEW_YORK_BOX_COORDS[0][1])&(df.pickup_longitude<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                   & (df.pickup_latitude>=NEW_YORK_BOX_COORDS[0][0]) & (df.pickup_latitude<=NEW_YORK_BOX_COORDS[1][0])\n",
    "                   & (df.dropoff_longitude>=NEW_YORK_BOX_COORDS[0][1])&(df.dropoff_longitude<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                   & (df.dropoff_latitude>=NEW_YORK_BOX_COORDS[0][0]) & (df.pickup_latitude<=NEW_YORK_BOX_COORDS[1][0])\n",
    "                   & (df.dropoff_latitude!=df.pickup_latitude) & (df.pickup_longitude!=df.dropoff_longitude)]\n",
    "    \n",
    "    # sample 2500 rows per file to match uber dataset\n",
    "    df = df.sample(2500)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    for csv_url in all_csv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        print(csv_url)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    # get dataframe from uber csv\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # rename column\n",
    "    df.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "    # convert pickup_datetime column to datetime\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df.pickup_datetime)\n",
    "    # drop unnecessary columns\n",
    "    df.drop([\"id\",\"key\"], axis=1, inplace=True)\n",
    "    # only select trips within new york coords & positive fare amount & positive passenger count\n",
    "    cleaned_df = df[(df.pickup_longitude>=NEW_YORK_BOX_COORDS[0][1])&(df.pickup_longitude<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                   & (df.pickup_latitude>=NEW_YORK_BOX_COORDS[0][0]) & (df.pickup_latitude<=NEW_YORK_BOX_COORDS[1][0])\n",
    "                   & (df.dropoff_longitude>=NEW_YORK_BOX_COORDS[0][1])&(df.dropoff_longitude<=NEW_YORK_BOX_COORDS[1][1])\n",
    "                   & (df.dropoff_latitude>=NEW_YORK_BOX_COORDS[0][0]) & (df.pickup_latitude<=NEW_YORK_BOX_COORDS[1][0])\n",
    "                   & (df.passenger_count>0) & (df.fare_amount>0)]\n",
    "    # drop unnecessary columns\n",
    "    cleaned_df = cleaned_df.drop([\"fare_amount\",\"passenger_count\"], axis=1).copy()\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df=pd.read_csv(csv_file,low_memory=False)\n",
    "    if df[\"HourlyPrecipitation\"] is\"NaN\":\n",
    "        df[\"HourlyPrecipitation\"]=df[\"DailyPrecipitation\"]/24\n",
    "    if df[\"HourlyWindSpeed\"] is \"NaN\":\n",
    "        df[\"HourlyWindSpeed\"]=df[\"DailyAverageWindSpeed\"]/24\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "    df[\"HourlyPrecipitation\"] = df[\"HourlyPrecipitation\"].replace([\"T\"],\"0.00\" )\n",
    "    df_new=df[[\"DATE\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    df_new.dropna(inplace=True)\n",
    "    return df_new\n",
    "    #replace T by 0.0,T means trace amount is greater than 0 but very small \n",
    "    #drop all blank rows and replace some blank rows by daily amount/24\n",
    "    #clean the date and choose three specific column for the following questions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df2=pd.read_csv(csv_file,low_memory=False)\n",
    "    if df2[\"DailyAverageWindSpeed\"] is \"NaN\":\n",
    "        df2[\"DailyAverageWindSpeed\"]=df2[\"HourlyWindSpeed\"]*24\n",
    "    df2[\"DATE\"] = pd.to_datetime(df2[\"DATE\"])\n",
    "    df_new2=df2[[\"DATE\",\"DailyAverageWindSpeed\"]]\n",
    "    df_new2.dropna(inplace=True)\n",
    "    return df_new2\n",
    "    #raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = [\"2009_weather.csv\" ,\"2010_weather.csv\",\"2011_weather.csv\",\"2012_weather.csv\",\"2013_weather.csv\",\"2014_weather.csv\",\"2015_weather.csv\"]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    hourlyid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATETIME,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    dailyid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATETIME,\n",
    "    DailyAverageWindSpeed FLOAT\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "   taxi_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   pickup_datetime DATETIME,\n",
    "   pickup_longitude DOUBLE,\n",
    "   pickup_latitude DOUBLE,\n",
    "   dropoff_longitude DOUBLE,\n",
    "   dropoff_latitude DOUBLE,\n",
    "   tip_amount FLOAT,\n",
    "   distance DOUBLE\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "   uber_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "   pickup_datetime DATETIME,\n",
    "   pickup_longitude DOUBLE,\n",
    "   pickup_latitude DOUBLE,\n",
    "   dropoff_longitude DOUBLE,\n",
    "   dropoff_latitude DOUBLE,\n",
    "   distance DOUBLE\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for table, data in table_to_df_dict.items():\n",
    "        df=data\n",
    "        df.to_sql(table, con=engine, if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c5c28e8",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"SELECT strftime ('%H',pickup_datetime) AS Hr, count(*) AS Ct\n",
    "FROM taxi_trips GROUP BY strftime ('%H',pickup_datetime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c03fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"most_popular_hour_taxi.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4390f5b",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837231f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"SELECT strftime('%w',pickup_datetime) AS Day, count(*) AS Ct\n",
    "FROM uber_trips GROUP BY strftime('%w',pickup_datetime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61258b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"most_popular_day_uber.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4dd6c3",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "What is the 95% percentile of distance traveled for all hired trips during July 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842db45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "WITH hired_trips AS (\n",
    "SELECT pickup_datetime, distance FROM taxi_trips \n",
    "WHERE strftime(\"%Y-%m\", pickup_datetime)=\"2013-07\" UNION ALL \n",
    "SELECT pickup_datetime, distance FROM uber_trips\n",
    "WHERE strftime(\"%Y-%m\", pickup_datetime)='2013-07'\n",
    ")\n",
    "SELECT distance from (SELECT\n",
    "    distance,\n",
    "    printf('%.2f', PERCENT_RANK() OVER( \n",
    "        ORDER BY distance \n",
    "    )) Rank FROM hired_trips ORDER BY Rank DESC)\n",
    "    WHERE Rank ='0.95' LIMIT 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b68d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95_percentile_distance.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a75cd283",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68336a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\" SELECT strftime('%Y-%m-%d', pickup_datetime), count(*), AVG(distance) FROM(\n",
    "SELECT pickup_datetime, distance FROM uber_trips WHERE strftime('%Y', pickup_datetime)='2009'\n",
    "UNION ALL\n",
    "SELECT pickup_datetime, distance FROM taxi_trips WHERE strftime('%Y', pickup_datetime)='2009') \n",
    "GROUP BY strftime('%Y-%m-%d', pickup_datetime) ORDER BY count(*) DESC\n",
    "LIMIT 10\n",
    ";\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"top_10_days_hired_rides_2009.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a009d1f",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0561ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "WITH pickup_datetime AS (\n",
    "SELECT strftime(\"%Y-%m-%d\", DATE) \n",
    "FROM daily_weather \n",
    "GROUP BY strftime(\"%Y-%m-%d\", DATE)\n",
    "ORDER by DailyAverageWindSpeed DESC  \n",
    "LIMIT 10\n",
    ")\n",
    "SELECT (SELECT COUNT(*) FROM taxi_trips where taxi_trips.pickup_datetime==pickup_datetime ) \n",
    "+ (SELECT COUNT(*) FROM uber_trips where uber_trips.pickup_datetime==pickup_datetime) AS total\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"top_10_windiest_days.sql\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd7aae93",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ccd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "WITH new_trips AS (\n",
    "SELECT strftime ('%Y-%m-%d %H',pickup_datetime) AS Hr, count(*) AS Ct FROM(\n",
    "SELECT pickup_datetime FROM uber_trips \n",
    "WHERE strftime('%Y-%m-%d',pickup_datetime) IN ('2012-10-22','2012-10-23','2012-10-24','2012-10-25','2012-10-26','2012-10-27','2012-10-28','2012-10-29','2012-10-30') \n",
    "UNION ALL\n",
    "SELECT pickup_datetime FROM taxi_trips\n",
    "WHERE strftime('%Y-%m-%d',pickup_datetime) IN ('2012-10-22','2012-10-23','2012-10-24','2012-10-25','2012-10-26','2012-10-27','2012-10-28','2012-10-29','2012-10-30') \n",
    ")\n",
    "GROUP BY strftime ('%Y-%m-%d %H',pickup_datetime)\n",
    "), new_weather AS(\n",
    "SELECT strftime ('%Y-%m-%d %H',date) AS Hr, HourlyPrecipitation, HourlyWindSpeed FROM hourly_weather \n",
    "WHERE strftime('%Y-%m-%d', date) IN ('2012-10-22','2012-10-23','2012-10-24','2012-10-25','2012-10-26','2012-10-27','2012-10-28','2012-10-29','2012-10-30') \n",
    "GROUP BY strftime ('%Y-%m-%d %H',date)\n",
    ")\n",
    "\n",
    "SELECT  new_trips.Hr, new_trips.Ct, coalesce(new_weather.HourlyPrecipitation, 0), coalesce(new_weather.HourlyWindSpeed, 0) \n",
    "FROM new_trips LEFT JOIN new_weather ON new_weather.Hr =new_trips.Hr\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"Hurricane_NYC.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n(QUERY_N):\n",
    "    df = pd.read_sql_query(QUERY_N, con=engine)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1faf2422",
   "metadata": {},
   "source": [
    "### Visualization 1\n",
    "Create an appropriate visualization for the first query/question in part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_popular_hour():\n",
    "    # get the data of hired taxi trips group by 24-hour\n",
    "    df=get_data_for_visual_n(QUERY_1)\n",
    "    # set hour as x-axis & count as y-axis\n",
    "    x=df[\"Hr\"]\n",
    "    y=df[\"Ct\"]\n",
    "    # label x & y axes\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    # set title\n",
    "    plt.title(\"Most Popular Hour of Yellow Taxi\")\n",
    "    # make bar graph\n",
    "    plt.bar(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719147bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_popular_hour()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d209ecde",
   "metadata": {},
   "source": [
    "### Visualization 2\n",
    "Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_dis_per_month():\n",
    "    # get avg distance per month from all trips\n",
    "    query=\"\"\"SELECT strftime ('%m',pickup_datetime) as Month, AVG(distance) as AvgDis FROM\n",
    "    (SELECT pickup_datetime, distance FROM uber_trips UNION ALL\n",
    "    SELECT pickup_datetime, distance FROM taxi_trips)\n",
    "    GROUP BY strftime ('%m',pickup_datetime)\"\"\"\n",
    "    df=get_data_for_visual_n(query)\n",
    "    # set month as x-axis & avg distance as y-axis\n",
    "    x=df[\"Month\"]\n",
    "    y=df[\"AvgDis\"]\n",
    "    # calculate the 90% confidence interval for avg distance\n",
    "    ci = 1.645 * np.std(y)/np.sqrt(len(y))\n",
    "    # label x & y axes\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Average Distance\")\n",
    "    # set title\n",
    "    plt.title(\"Average Distance Per Month\")\n",
    "    # plot avg distance on month\n",
    "    plt.plot(x,y)\n",
    "    # plot 90% confidence interval\n",
    "    plt.fill_between(x, (y-ci), (y+ci), color='b', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dis_per_month()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd83aca",
   "metadata": {},
   "source": [
    "### Visualization 3\n",
    "Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5095f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lat/long coordinate boxes around three airports\n",
    "lga_coords=((-73.890314,40.766578),(-73.862762,40.774314))\n",
    "jfk_coords=((-73.826635,40.621911),(-73.743036,40.673748))\n",
    "ewr_coords=((-74.200639,40.671241),(-74.150170,40.715754))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591acc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dropoffs_for_lga_airport():\n",
    "    # select trips with dropoffs at lga airport from all trips, count and group by day of week\n",
    "    query=\"\"\"SELECT strftime(\"%w\", pickup_datetime) AS Day, count(*) AS Ct FROM \n",
    "    (SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM uber_trips \n",
    "    WHERE dropoff_longitude>=-73.890314 AND dropoff_longitude<=-73.862762 AND dropoff_latitude>=40.766578 AND dropoff_latitude<=40.774314\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM taxi_trips\n",
    "    WHERE dropoff_longitude>=-73.890314 AND dropoff_longitude<=-73.862762 AND dropoff_latitude>=40.766578 AND dropoff_latitude<=40.774314\n",
    "    )\n",
    "    GROUP BY strftime(\"%w\", pickup_datetime)\"\"\"\n",
    "    df=get_data_for_visual_n(query)\n",
    "    # choose day as x-axis & counts as y-axis\n",
    "    x=df[\"Day\"]\n",
    "    y=df[\"Ct\"]\n",
    "    # label x & y axes\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    # set title\n",
    "    plt.title(\"Most Popular Day of LGA\")\n",
    "    # make bar graph\n",
    "    plt.bar(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed070626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dropoffs_for_jfk_airport():\n",
    "    # select trips with dropoffs at jfk airport from all trips, count and group by day of week\n",
    "    query=\"\"\"SELECT strftime(\"%w\", pickup_datetime) AS Day, count(*) AS Ct FROM \n",
    "    (SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM uber_trips \n",
    "    WHERE dropoff_longitude>=-74.200639 AND dropoff_longitude<=-73.743036 AND dropoff_latitude>=40.621911 AND dropoff_latitude<=40.673748\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM taxi_trips\n",
    "    WHERE dropoff_longitude>=-73.826635 AND dropoff_longitude<=-73.743036 AND dropoff_latitude>=40.621911 AND dropoff_latitude<=40.673748\n",
    "    )\n",
    "    GROUP BY strftime(\"%w\", pickup_datetime)\"\"\"\n",
    "    df=get_data_for_visual_n(query)\n",
    "    # set day as x-axis & count as y-axis\n",
    "    x=df[\"Day\"]\n",
    "    y=df[\"Ct\"]\n",
    "    # label x & y axes\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    # set title\n",
    "    plt.title(\"Most Popular Day of JFK\")\n",
    "    # make bar graph\n",
    "    plt.bar(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dropoffs_for_ewr_airport():\n",
    "     # select trips with dropoffs at ewr airport from all trips, count and group by day of week\n",
    "    query=\"\"\"SELECT strftime(\"%w\", pickup_datetime) AS Day, count(*) AS Ct FROM \n",
    "    (SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM uber_trips \n",
    "    WHERE dropoff_longitude>=-74.150170 AND dropoff_longitude<=-73.826635 AND dropoff_latitude>=40.671241 AND dropoff_latitude<=40.715754\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM taxi_trips\n",
    "    WHERE dropoff_longitude>=-74.150170 AND dropoff_longitude<=-73.826635 AND dropoff_latitude>=40.671241 AND dropoff_latitude<=40.715754\n",
    "    )\n",
    "    GROUP BY strftime(\"%w\", pickup_datetime)\"\"\"\n",
    "    df=get_data_for_visual_n(query)\n",
    "    # set day as x axis & counts as y axis\n",
    "    x=df[\"Day\"]\n",
    "    y=df[\"Ct\"]\n",
    "    # label x & y axes\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    # set title\n",
    "    plt.title(\"Most Popular Day of EWR\")\n",
    "    # make bar graph\n",
    "    plt.bar(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edb9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dropoffs_for_lga_airport()\n",
    "plot_dropoffs_for_jfk_airport()\n",
    "plot_dropoffs_for_ewr_airport()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5741607f",
   "metadata": {},
   "source": [
    "### Visualization 5\n",
    "Create a scatter plot that compares tip amount versus distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af84d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tip_amount_distance():\n",
    "    query=\"\"\"SELECT tip_amount as Tip_Amt, distance as Distance FROM taxi_trips\"\"\"\n",
    "    # get data of distance & tip amount from taxi trips\n",
    "    df=get_data_for_visual_n(query)\n",
    "    # set distance as x-axis & tip_amount as y-axis\n",
    "    x=df[\"Distance\"]\n",
    "    y=df[\"Tip_Amt\"]\n",
    "    # label x & y axes\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Tip_Amt\")\n",
    "    # set title\n",
    "    plt.title(\"Tip Amount versus Distance\")\n",
    "    # make scatter plot\n",
    "    plt.scatter(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tip_amount_distance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
